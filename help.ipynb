{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Informer 模型的简要介绍\n",
    "- Informer 是一种用于长序列时间序列预测的深度学习模型\n",
    "- 它采用了自注意力机制,但通过创新的 ProbSparse 自注意力来降低计算复杂度\n",
    "- 相比传统 Transformer,Informer 能够处理更长的输入序列,并生成更长的预测序列,它在多个长序列预测任务上取得了优异的性能\n",
    "- Informer 模型于 2020 年 11 月首次在 arXiv 上发表\n",
    "- 论文标题为 \"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\"\n",
    "- 作者来自中国人民大学和清华大学\n",
    "- Informer 是一个专门针对长序列时间序列预测优化的 Transformer 变体,在 2020 年底提出并引起了广泛关注。"
   ],
   "id": "33b57c880ca79d22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# utils.masking.py",
   "id": "da1d753067f39508"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> TriangularCausalMask,ProbMask\n",
    "- 这两个类都是用来实现概率掩码（Probability Mask）的。概率掩码是一种在生成文本时常用的技术，它有助于减少生成的文本中重复或不相关的单词。\n",
    "- TriangularCausalMask 类实现了一个对角线因果掩码（Diagonal Causal Mask），其中对角线因果掩码用于引导生成过程。在这个例子中，类接收三个参数：batch size (B)，序列长度（L），以及设备（device）。它使用 PaddlePaddle 的 extract_loc 函数生成一个对角线因果掩码，并将其放在设备上。然后，将此掩码暴露给外界的属性为 mask\n",
    "- ProbMask 类实现了一个概率掩码，它基于一个指示器（indicator）张量。这个指示器张量用于确定哪个单词应该被保留，哪个单词应该被掩码掉。在这个例子中，类接收五个参数：batch size (B)，每个时间步长的嵌入维度（H），序列长度（L），掩码的索引（index），以及嵌入的得分（scores）。这个类通过使用 PaddlePaddle 的 triu 函数创建一个上三角形张量，然后将其扩展并转换为概率掩码。这个概率掩码被暴露给外界的属性为 mask。"
   ],
   "id": "c44b6aa339a2bbb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# models.attention.py\n",
    "自注意力机制（Self-Attention）使得模型能够自动捕捉输入序列中各个元素之间的关系，从而更好地理解输入序列。这种机制使得模型能够并行处理输入序列中的所有元素，避免了RNN/LSTM等序列模型中存在的计算顺序依赖问题，从而更高效地进行处理。自注意力机制在自然语言处理领域的应用非常广泛，例如机器翻译、语音识别、文本分类等任务中都有很好的效果。"
   ],
   "id": "648baf8c584b034f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# models.embed.py\n",
    "- 词嵌入是一种将自然语言转换为向量空间模型的技术，它可以将一个单词表示为一个实数向量。这种技术有着广泛的应用，包括但不限于：\n",
    "- 词向量表示：将单词表示为低维向量，可以用于文本分类、聚类、信息检索等任务。\n",
    "- 语言模型：通过训练一个神经网络来预测一个单词是否为一个给定上下文中的下一个单词，可以用于文本生成、机器翻译等任务。\n",
    "- 文本分类：使用 Transformer 模型作为特征提取器，将文本表示为向量，可以用于情感分析、关键词提取等任务。\n",
    "- 命名实体识别：使用 Transformer 模型作为识别器，可以用于识别文本中的人名、地名、组织名等实体。\n",
    "- 总之，Transformer 词嵌入技术在自然语言处理领域中有着非常广泛的应用，它可以帮助我们更好地理解和处理自然语言文本数据。"
   ],
   "id": "8d7c2d7098a5844b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# models.encoder.py\n",
    "编码器"
   ],
   "id": "cbfa9ba0fb0a716c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# models.decoder.py\n",
    "解码器\n"
   ],
   "id": "fcf80c6b3b4cd028"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
